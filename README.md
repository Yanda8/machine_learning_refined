# Machine Learning Refined Jupyter notebooks 

This repository contains supplementary Python files associated the texbook [Machine Learning Refined](http://www.mlrefined.com) published by Cambridge University Press, as well as a set of interactive notes (made from Jupyter notebooks) that served as an early draft for the second edition of the text.        
      
## Notes

### Chapter 2: Zero order / derivative free optimization

[2.1  Introduction](https://jermwatt.github.io/mlrefined/notes/2_Zero_order_methods/2_1_Introduction.html)  
[2.2 Zero order optimiality conditions](https://jermwatt.github.io/mlrefined/notes/2_Zero_order_methods/2_2_Zero.html)  
[2.3 Global optimization](https://jermwatt.github.io/mlrefined/notes/2_Zero_order_methods/2_3_Global.html)    
[2.4 Local optimization techniques](https://jermwatt.github.io/mlrefined/notes/2_Zero_order_methods/2_4_Local.html)   
[2.5 Random search methods](https://jermwatt.github.io/mlrefined/notes/2_Zero_order_methods/2_5_Random.html)
 
### Chapter 3: First order optimization methods

[3.1 Introduction](https://jermwatt.github.io/mlrefined/notes/3_First_order_methods/3_1_Introduction.html)  

[3.2 The first order optimzliaty condition](https://jermwatt.github.io/mlrefined/notes/3_First_order_methods/3_2_First.html) 

[3.3 The anatomy of lines and hyperplanes](https://jermwatt.github.io/mlrefined/notes/3_First_order_methods/3_3_Hyperplane.html)
[3.4 The anatomy of first order Taylor series approximations](https://jermwatt.github.io/mlrefined/notes/3_First_order_methods/3_4_Tangent.html)
[3.5 Automatic differentiation and autograd](https://jermwatt.github.io/mlrefined/notes/3_First_order_methods/3_5_Automatic.html)   
[3.6 Gradient descent](https://jermwatt.github.io/mlrefined/notes/3_First_order_methods/3_6_Descent.html)   
[3.7 Two problems with the negative gradient direction](https://jermwatt.github.io/mlrefined/notes/3_First_order_methods/3_7_Problems.html)   
[3.8 Momentum acceleration](https://jermwatt.github.io/mlrefined/notes/3_First_order_methods/3_8_Momentum.html)   
[3.9 Normalized gradient descent procedures](https://jermwatt.github.io/mlrefined/notes/3_First_order_methods/3_9_Normalized.html)   
[3.10 Advanced first order methods](https://jermwatt.github.io/mlrefined/notes/3_First_order_methods/3_10_Advanced.html)   
[3.11 Mini-batch methods](https://jermwatt.github.io/mlrefined/notes/3_First_order_methods/3_11_Minibatch.html)   
[3.12 Conservative steplength rules](https://jermwatt.github.io/mlrefined/notes/3_First_order_methods/3_12_Conservative.html)  

### Chapter 4: Second order optimization methods

4.1  Introduction <br>
[4.2  The anatomy of quadratic functions](https://jermwatt.github.io/mlrefined/notes/4_Second_order_methods/4_2_Quadratic.html)   
[4.3 Curvature and the second order optimality condition](https://jermwatt.github.io/mlrefined/notes/4_Second_order_methods/4_3_Second.html)   
[4.4 Newton's method](https://jermwatt.github.io/mlrefined/notes/4_Second_order_methods/4_4_Newtons.html)   
[4.5 Two fundamental problems with Newton's method](https://jermwatt.github.io/mlrefined/notes/4_Second_order_methods/4_5_Problems.html)   
4.6 Quasi-newton's methods 

### Chapter 5: Linear regression

5.1 Introduction <br>
[5.2 Least squares regression](https://jermwatt.github.io/mlrefined/notes/5_Linear_regression/5_2_Least.html)   
[5.3 Least absolute deviations](https://jermwatt.github.io/mlrefined/notes/5_Linear_regression/5_3_Absolute.html)   
[5.4 Regression metrics](https://jermwatt.github.io/mlrefined/notes/5_Linear_regression/5_4_Metrics.html)   
[5.5 Weighted regression](https://jermwatt.github.io/mlrefined/notes/5_Linear_regression/5_5_Weighted.html)   
[5.6 Multi-output regression](https://jermwatt.github.io/mlrefined/notes/5_Linear_regression/5_6_Multi.html)  

### Chapter 6: Linear two-class classification

6.1 Introduction <br>
[6.2 Logistic regression and the cross-entropy cost](https://jermwatt.github.io/mlrefined/notes/6_Linear_twoclass_classification/6_2_Cross_entropy.html)   
[6.3 Logistic regression and the softmax cost](https://jermwatt.github.io/mlrefined/notes/6_Linear_twoclass_classification/6_3_Softmax.html)   
[6.4 The perceptron](https://jermwatt.github.io/mlrefined/notes/6_Linear_twoclass_classification/6_4_Perceptron.html)   
[6.5 Support vector machines](https://jermwatt.github.io/mlrefined/notes/6_Linear_twoclass_classification/6_5_SVMs.html)   
[6.6 Categorical labels](https://jermwatt.github.io/mlrefined/notes/6_Linear_twoclass_classification/6_6_Categorical.html)   
[6.7 Comparing two-class schemes](https://jermwatt.github.io/mlrefined/notes/6_Linear_twoclass_classification/6_7_Comparison.html)   
[6.8 Quality metrics](https://jermwatt.github.io/mlrefined/notes/6_Linear_twoclass_classification/6_8_Metrics.html)   
[6.9 Weighted two-class classification](https://jermwatt.github.io/mlrefined/notes/6_Linear_twoclass_classification/6_9_Weighted.html)  

### Chapter 7: Linear multi-class classification

7.1 Introduction <br>
[7.2 One-versus-All classification](https://jermwatt.github.io/mlrefined/notes/7_Linear_multiclass_classification/7_2_OvA.html)   
[7.3 The multi-class perceptron](https://jermwatt.github.io/mlrefined/notes/7_Linear_multiclass_classification/7_3_Perceptron.html)   
[7.4 Comparing multi-class schemes](https://jermwatt.github.io/mlrefined/notes/7_Linear_multiclass_classification/7_4_Comparison.html)   
[7.5 The categorical cross-entropy cost](https://jermwatt.github.io/mlrefined/notes/7_Linear_multiclass_classification/7_5_Categorical.html)   
[7.6 Multi-class quality metrics](https://jermwatt.github.io/mlrefined/notes/7_Linear_multiclass_classification/7_6_Metrics.html)  


### Chapter 8: Unsupervised learning

8.1 Introduction <br>
[8.2 Spanning sets and vector algebra](https://jermwatt.github.io/mlrefined/notes/8_Linear_unsupervised_learning/8_2_Spanning.html)   
[8.3 Learning proper spanning sets](https://jermwatt.github.io/mlrefined/notes/8_Linear_unsupervised_learning/8_3_PCA.html)   
[8.4 The linear Autoencoder](https://jermwatt.github.io/mlrefined/notes/8_Linear_unsupervised_learning/8_4_Autoencoder.html)   
[8.5 The class PCA solution](https://jermwatt.github.io/mlrefined/notes/8_Linear_unsupervised_learning/8_5_Classic.html)   
[8.6 Recommender systems](https://jermwatt.github.io/mlrefined/notes/8_Linear_unsupervised_learning/8_6_Recommender.html)  
[8.7 K-means clustering](https://jermwatt.github.io/mlrefined/notes/8_Linear_unsupervised_learning/8_7_Kmeans.html)   
[8.8 Matrix factorization techniques](https://jermwatt.github.io/mlrefined/notes/8_Linear_unsupervised_learning/8_8_Factorization.html)  

### Chapter 9: Principles of feature selection and engineering

9.1 Introduction <br>
[9.2 Histogram-based features](https://jermwatt.github.io/mlrefined/notes/9_Feature_engineer_select/9_2_Histogram.html)   
[9.3 Standard normalization and feature scaling](https://jermwatt.github.io/mlrefined/notes/9_Feature_engineer_select/9_3_Scaling.html)   
[9.4 Imputing missing values](https://jermwatt.github.io/mlrefined/notes/9_Feature_engineer_select/9_4_Cleaning.html)   
[9.5 PCA-sphereing](https://jermwatt.github.io/mlrefined/notes/9_Feature_engineer_select/9_5_PCA_sphereing.html)   
[9.6 Feature selection via boosting](https://jermwatt.github.io/mlrefined/notes/9_Feature_engineer_select/9_6_Boosting.html)   
[9.7 Feature selection via regularization](https://jermwatt.github.io/mlrefined/notes/9_Feature_engineer_select/9_7_Regularization.html)  

### Chapter 10: Introduction to nonlinear learning

10.1 Introduction <br>
[10.2 Nonlinear regression](https://jermwatt.github.io/mlrefined/notes/10_Nonlinear_intro/10_2_Regression.html)  
[10.3 Nonlinear multi-output regression](https://jermwatt.github.io/mlrefined/notes/10_Nonlinear_intro/10_3_MultReg.html)  
[10.4 Nonlinear two-class classification](https://jermwatt.github.io/mlrefined/notes/10_Nonlinear_intro/10_4_Twoclass.html)  
[10.5 Nonlinear multi-class classification](https://jermwatt.github.io/mlrefined/notes/10_Nonlinear_intro/10_5_Multiclass.html)  
[10.6 Nonlinear unsupervised learning](https://jermwatt.github.io/mlrefined/notes/10_Nonlinear_intro/10_6_Unsupervised.html)  

### Chapter 11: Principles of feature learning

11.1 Introduction <br>
11.2 Universal approximators <br>
11.3 Universal approximation of real data  <br>
11.4 Naive cross-validation  <br>
11.5 Efficient cross-validation via boosting  <br>
11.6 Efficient cross-validation via regularization  <br>
11.7 Testing data <br>
11.8 Which universal approximator works best in practice? <br>
11.9 Bagging cross-validated models <br>
11.10 K-folds cross-validation <br>
11.11 When feature learning fails <br>
11.12 Conclusion <br>

### Chapter 12: Kernels

12.1 Introduction <br>
12.2 The variety of kernel-based learners <br>
12.3 The kernel trick  <br>
12.4 Kernels as similarity measures <br>  
12.5 Scaling kernels <br>
  
### Chapter 13: Fully connected networks

13.1 Introduction <br>
[13.2 Fully connected networks](https://jermwatt.github.io/mlrefined/notes/13_Multilayer_perceptrons/13_2_Multi_layer_perceptrons.html)  
[13.3 Optimization issues](https://jermwatt.github.io/mlrefined/notes/13_Multilayer_perceptrons/13_4_Optimization.html)  
13.4 Activation functions  
13.5 Backpropogation  
[13.6 Batch normalization](https://jermwatt.github.io/mlrefined/notes/13_Multilayer_perceptrons/13_7_Batch_normalization.html)  
[13.7 Early-stopping](https://jermwatt.github.io/mlrefined/notes/13_Multilayer_perceptrons/13_7_early_stopping.html)  

### Chapter 14: Tree-based learners

14.1 Introduction <br>
14.2 Varieties of tree-based learners <br>
14.3 Regression trees  <br>
14.4 Classification trees  <br>
14.5 Gradient boosting  <br>
14.6 Random forests  <br>
14.7 Cross-validating individual trees  <br>


## Install instructions
To successfully run the Jupyter notebooks contained in this repo we highly recommend downloading the [Anaconda Python 3 distribution](https://www.anaconda.com/download/#macos).  Many of these notebooks also employ the Automatic Differentiator [autograd](https://github.com/HIPS/autograd) which can be installed by typing the following command at your terminal
      
      pip install autograd
      
With minor adjustment users can also run these notebooks using the GPU/TPU extended version of autograd  [JAX](https://github.com/google/jax).

Note: to pull a minimial sized clone of this repo (including only the most recent commit) use a shallow pull as follows
      
      git clone --depth 1 https://github.com/jermwatt/mlrefined.git

--- 
This repository is in active development by [Jeremy Watt](mailto:jeremy@dgsix.com) and [Reza Borhani](mailto:reza@dgsix.com) - please do not hesitate to reach out with comments, questions, typos, etc.
